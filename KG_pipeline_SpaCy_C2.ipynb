{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Knowledge Graph from the output of the NER & RE models\n",
    "This time, using a SpaCy implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Component 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions on how to work with Neo4j alongside this code\n",
    "\n",
    "Before you start, download neo4j desktop from https://neo4j.com/download/\n",
    "\n",
    "Then, create a project called \"Text_Mining\" (or whatever name you want, since this name is not used within this code). \n",
    "\n",
    "Click \"Add\" -> \"Local DMBS\" and name it \"Text_Mining_Neo4j\" (even though this name is also not used within this code) while setting the password to \"bilalbroski1\" (IMPORTANT!)\n",
    "\n",
    "Then you have to click the blue \"Start\" button and wait for the server to start. After it is done loading, click the blue \"Open\" button. This will cause the Neo4j Browser to open. \n",
    "\n",
    "In Neo4j browser, run the command \"MATCH (n) RETURN n\" to show the complete KG, and run \"MATCH (n) DETACH DELETE n\" to completely empty the DMBS.\n",
    "\n",
    "If any of this does not work, follow the steps in the \"Download Neo4j\" section of https://youtu.be/8jNPelugC2s?si=QV898-ggdLIq9XPk&t=597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os, os.path\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from unidecode import unidecode\n",
    "from num2words import num2words\n",
    "import pickle\n",
    "# from copy import deepcopy\n",
    "import json\n",
    "\n",
    "# # imports for loading the NER model using flair\n",
    "# from flair.data import Sentence\n",
    "# from flair.models import SequenceTagger\n",
    "\n",
    "# import for loading the NER model using SpaCy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # load the NER model using FLAIR\n",
    "# # custom_ner_model = SequenceTagger.load(r'flair_models\\best-model.pt')\n",
    "\n",
    "# # Load the NER model using SpaCy\n",
    "# custom_ner_model = spacy.load(\"model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified verison of what Bilal wrote on 26-01-2024\n",
    "def augmented_text(data):\n",
    "    augmented_data = []\n",
    "    for example in data:\n",
    "        old_text = example['text']\n",
    "        new_text = ''\n",
    "        cur_index = 0\n",
    "        for entity in example['entities']:\n",
    "            start = entity[0]\n",
    "            end = entity[1]\n",
    "            label = entity[2]\n",
    "\n",
    "            if cur_index < start:\n",
    "                new_text += old_text[cur_index:start]\n",
    "                new_text += f'[{label}]{old_text[start:end]}[{label}]'\n",
    "                cur_index = end\n",
    "            else:\n",
    "                new_text += f'[{label}]{old_text[start:end]}[{label}]'\n",
    "                cur_index = end\n",
    "            \n",
    "        augmented_data.append(new_text)\n",
    "    \n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to which the text(s) can be added\n",
    "texts = []\n",
    "\n",
    "# only execute if manually_select_texts is set to True\n",
    "manually_select_texts = False\n",
    "\n",
    "if manually_select_texts:\n",
    "\n",
    "    # Set parameters for loading texts\n",
    "\n",
    "    # set to True in case you want to select a single specific text\n",
    "    select_text = True\n",
    "\n",
    "    # index of the specific text that you want to select\n",
    "    text_i = 369  # index of text for Thomas Alun Lockyer = 192; Mark Maria Hubertus Flekken = 369\n",
    "\n",
    "    # if select_text is set to False, there will be nr_texts selected\n",
    "    nr_texts = 2 # max number of texts is 1031\n",
    "\n",
    "    # import the texts that have to be converted into a KG (=Knowledge Graph)\n",
    "\n",
    "    texts = []\n",
    "\n",
    "    if select_text: # only select the text with index text_i\n",
    "        with open(f'./data/footballer_{text_i}.txt', 'r') as f:\n",
    "            contents = f.read()\n",
    "            texts.append(contents)\n",
    "        f.close()\n",
    "        print(f\"The text with index number {text_i} has successfully been imported\")\n",
    "\n",
    "    else: # select the first nr_texts texts\n",
    "        for i in range(nr_texts):\n",
    "            with open(f'./data/footballer_{i}.txt', 'r') as f:\n",
    "                contents = f.read()\n",
    "                texts.append(contents)\n",
    "            f.close()\n",
    "\n",
    "        print(f\"{len(texts)} texts have been successfully imported\")\n",
    "\n",
    "    print(texts)\n",
    "\n",
    "else:\n",
    "    text_ids = []\n",
    "\n",
    "    with open(r'predictions_C2.json','r') as data_file:    \n",
    "        data = json.load(data_file)\n",
    "        # for v in data:#[0].values():\n",
    "        #     if v['id'] not in text_ids:\n",
    "        #         # aug_text = augmented_text(v['text'])\n",
    "        #         texts.append(v['text'])\n",
    "        #         text_ids.append(v['id'])\n",
    "        texts = augmented_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"On [DATE]2/18/87,[DATE] at [TIME]0001 hours[TIME], during normal steady state operation, ([LOCATION]Mode 1[LOCATION], at 100 percent power) and no rod motion in progress, [EMPLOYEE]the Control Room operators[EMPLOYEE] observed a decrease in reactor power and that No. 2 control rod's position lights indicated that it had dropped. [EMPLOYEE]The operators[EMPLOYEE] immediately began to reduce generator load to restore the main coolant system Tave per plant procedure. At 0003 hours the reactor protection system initiated an automatic scram as the result of a [UNEXPECTED EVENT]high main coolant pressure condition[UNEXPECTED EVENT]. The [UNEXPECTED EVENT]high main coolant pressure[UNEXPECTED EVENT] occurred because the load reduction by the [EMPLOYEE]operator[EMPLOYEE] overcompensated for the power reduction from the dropped rod. The NRC was notified via the ENS at [TIME]0101 hours[TIME] [DATE]February 18, 1987[DATE].  <br /><br />The root cause of this event was determined to be [CAUSE]a procedure inadequacy[CAUSE]\",\n",
       " 'On [DATE]July 6, 1987[DATE], with the plant in [LOCATION]Mode 2[LOCATION] at 5 E-6 amps reactor power, [UNEXPECTED EVENT]a reactor scram on high main coolant pressure[UNEXPECTED EVENT] occurred during a main turbine overspeed trip test. The primary cause of this event has been attributed to [CAUSE]personnel error[CAUSE]. [UNEXPECTED EVENT]The turbine operator (at the turbine pedestal) used an excessive amount of steam to increase turbine speed[UNEXPECTED EVENT] which initiated a steam flow, reactor power mismatch. A [ACTIVITY]second factor was the[ACTIVITY] lack of communication between the turbine [EMPLOYEE]operator[EMPLOYEE] and the reactor [EMPLOYEE]operator[EMPLOYEE] which delayed timely compensatory actions. <br /><br /> [CAUSE]Plant response was normal[CAUSE] following the scram. Corrective actions were taken to reinstruct [EMPLOYEE]personnel[EMPLOYEE] and establish a direct communications link between the turbine [EMPLOYEE]operator[EMPLOYEE] and reactor [EMPLOYEE]operator[EMPLOYEE]',\n",
       " 'On [DATE]3/22/88,[DATE] at [TIME]0042[TIME], with the Plant operating in Mode 1 at 100% power, an [UNEXPECTED EVENT]automatic reactor scram[UNEXPECTED EVENT] occurred when power to the scram amplifiers in the Nuclear Instrument (NI) cabinet \"A\" was lost. The loss of power to NI Cabinet \"A\" was a result of a failed Sola Transformer (Type CV) within the NI Cabinet. The loss of power produces a loss of indication of NI channels 1, 3 & 6. An ENS phone call was made at [TIME]0141[TIME] on [DATE]3/22/88[DATE]',\n",
       " 'On [DATE]March 26, 1988[DATE], while at 85% power and returning to normal full power operation, there was an alarm for SG high-low level, immediately followed by an [UNEXPECTED EVENT]automatic reactor scram[UNEXPECTED EVENT] and turbine trip on low SG levels. Just prior to this event, loop No. 2 feedwater flow began oscillating and loop No. 4 feedwater flow started decreasing. As a result, the levels in No. 2 and 4 steam generators were decreasing rapidly towards the trip setpoint. [EXPECTED EVENT]The reactor scram[EXPECTED EVENT] and turbine trip occurred at [TIME]0529 hours[TIME]. The NRC was notified via ENS at 0603 hours.<br /><br /> As part of the emergency procedures, the [EMPLOYEE]operators[EMPLOYEE]',\n",
       " 'At [TIME]2323 hours[TIME], 17 [ACTIVITY]May 1988,[ACTIVITY] in [LOCATION]Mode 1[LOCATION] at full power operation (100%), a loss of generator field excitation resulted in automatic turbine governor response and subsequent relay action which tripped BK-1 and BK-2 (reactor trip breakers). The loss of generator field excitation resulted from automatic tripping of the AC feed to the static exciter. The Harriman (Z-126) transmission line also deenergized during this event. Loss of both generator excitation and one of two transmission lines resulted in a loss of flow from three of four main coolant pumps (MCPs). [EMPLOYEE]The operators[EMPLOYEE] secured MCP #1 approximately two minutes after plant trip and established natural circulation. By [TIME]2330 hours[TIME] the Z-126 line had been re-energized. By 2345 hours the electrical busses had been cross-tied and restart of the MCPs had commenced. At [TIME]0005 hours[TIME], 18 [ACTIVITY]May 1988, all four MCPs were operating[ACTIVITY]. All automatic safety systems functioned as designed: the plant emergency diesel generators No 2 and 3 started as required. <br /><br /> The root cause of this event was [CAUSE]a failure of the Field Overvoltage Protection unit circuit board[CAUSE]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escapeRegExp(text):\n",
    "    \"\"\"\n",
    "    Replace all of the opening brackets with \"\\[\" to escape the special characters in the regrex.\n",
    "    This is required to prevent unterminated character sets.\n",
    "    If the unterminated character sets error still occurs, try running the commented out lines as well.\n",
    "    Source: https://stackoverflow.com/questions/54135606/python-re-error-unterminated-character-set-at-position\n",
    "    \"\"\"\n",
    "    edited_text = text.replace(\"[\", \"\\[\")\n",
    "    # edited_text = edited_text.replace(\"{\", \"\\{\")\n",
    "    # edited_text = edited_text.replace(\"(\", \"\\(\")\n",
    "    # edited_text = edited_text.replace(\")\", \"\\)\")\n",
    "    return edited_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_texts = map(escapeRegExp, texts) # where the \"texts\" variable contains the augmented texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_var_name(var_name: str):\n",
    "    \"\"\"\n",
    "    Check string for violations of the Cypher variable name conventions,\n",
    "    namely that it must always start with a letter.\n",
    "    If the string starts with an integer, replace it with the word for this integer.\n",
    "    For example, 2_october becomes two_october.\n",
    "    Remove apostrophes\n",
    "    The handleable_case parameter is set to True if the input is a handleable case. If not,\n",
    "    handleable_case will be set to False, which will lead to the abortion of the current iteration\n",
    "    in which fix_var_name() is being used.\n",
    "    \"\"\"\n",
    "    handleable_case = True # The case is assumed to be handleable\n",
    "\n",
    "    # if the case is unhandable, handleable_case will be set to False\n",
    "    if len(var_name)==1: # for variable names with a length of 1 \n",
    "        handleable_case = False\n",
    "    # if len(var_name) > 30: # for variable names that are extremely long\n",
    "    #     handleable_case = False \n",
    "\n",
    "    if len(var_name) > 0 and var_name[0].isdigit(): # this condition might cause the original error to occur again!!\n",
    "        i_final_int = 0 # starts at zero to prevent index out of range error from occuring at var_name[i_final_int].isdigit() in the while statement\n",
    "        \n",
    "        while ( var_name[i_final_int].isdigit() ) and ( i_final_int < len(var_name) -1 ):\n",
    "            i_final_int += 1\n",
    "        \n",
    "        if i_final_int > 0:\n",
    "            var_name = num2words( var_name[ : i_final_int] ) + var_name[i_final_int : ]\n",
    "            var_name = var_name.replace(\" \", \"_\").replace(\"-\", \"_\").lower() \n",
    "            \n",
    "    return var_name, handleable_case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate creation of Cypher CREATE query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(entity_str: str):\n",
    "    \"\"\"\n",
    "    Takes a string that represents an entity in the create_entity_query function\n",
    "    and removes symbols that cause problems when creating a Neo4j query.\n",
    "\n",
    "    Also look at the if statement (\"if i_final_int > 0\") within the fix_var_name() function.\n",
    "    \"\"\"\n",
    "    entity_str = entity_str.replace(\"-\", \"_\")\n",
    "\n",
    "    entity_str = entity_str.replace(\" \", \"_\").lower()\n",
    "    \n",
    "    entity_str = entity_str.replace(\"'\", \"\")\n",
    "    entity_str = entity_str.replace('\"','')\n",
    "    entity_str = entity_str.replace(\".\", \"\")\n",
    "    entity_str = entity_str.replace(\",\", \"\")\n",
    "    entity_str = entity_str.replace(\";\", \"\")\n",
    "    entity_str = entity_str.replace(\"(\", \"\")\n",
    "    entity_str = entity_str.replace(\")\", \"\")\n",
    "    entity_str = entity_str.replace(\"&\", \"\")\n",
    "    entity_str = entity_str.replace(\"\\\\\", \"\")\n",
    "    entity_str = entity_str.replace(\"[\", \"\")\n",
    "    entity_str = entity_str.replace(\"]\", \"\")\n",
    "\n",
    "    entity_str = entity_str.strip(\"_\")\n",
    "\n",
    "    return entity_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_str(entity_str: str):\n",
    "    \"\"\"\n",
    "    Takes the extracted string in the create_entity_query function, and modifies it\n",
    "    according to the naming conventions of neo4j.\n",
    "    \"\"\"\n",
    "    variable_str = entity_str.replace(\" \", \"_\").lower() \n",
    "    variable_str, handleable = fix_var_name(variable_str)\n",
    "    entity_str = entity_str.lower().title()\n",
    "\n",
    "    # entity_str = entity_str.replace('\"','').replace(\"'\",\"\")\n",
    "\n",
    "    # apply clean_str() function\n",
    "    variable_str = clean_str(variable_str)\n",
    "    entity_str = clean_str(entity_str)\n",
    "\n",
    "    return variable_str, entity_str, handleable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entity_query(augmented_texts: list, suppress_warnings: bool=True):\n",
    "    \"\"\"\n",
    "    Creates a Cypher query that creates the entity part of a Neo4j Database \n",
    "    using the entities from the augmented_texts list.\n",
    "    \n",
    "    Inputs:\n",
    "    - augmented_texts: list containing the annotated texts that have to be converted into a KG\n",
    "    - suppress_warnings: If True, the warnings about texts that the function skips over will be suppressed.\n",
    "\n",
    "    Assumptions: \n",
    "    - If entity tags are nested, this is never with a depth higher than 1. This means that the\n",
    "        nested entities should always be of the shape \"[TAG_1]entity1_pt1 [TAG_2]entity2[TAG2] entity1_pt1[TAG_1]\"\n",
    "    \n",
    "    Output: \n",
    "    - Each query-line has the following format: (variable_name:ENTITY_TAG{name: 'name'})\n",
    "    - In this format, each variable_name is completely decapitalized and all \n",
    "        whitespaces have been replaced by underscores.\n",
    "    - the name that is within the curly brackets of each entity has each individual word capitalized, \n",
    "        but that is the only preprocessing performed on it.\n",
    "    \"\"\"\n",
    "    # initialise a set to which all query-strings are added, and that can later be concatenated into a single string.\n",
    "    # This helps preventing duplicates\n",
    "    query_set = set()\n",
    "    # Initialise a set that keeps track of the node labels. This prevents the following error from occuring:\n",
    "    # \"\"\" {message: Variable `akanji` already declared (line 256, column 2 (offset: 10988)) \"\"\" \n",
    "    node_label_set = set()\n",
    "    # initialise variable that prevents a closing tag from being investigated by skipping the next iteration of the current for loop if it equals True\n",
    "    skip_iteration = False \n",
    "    # initialise variable that skips over an entire nested tag structure after it has been analyzed\n",
    "    skip_nested_structure = 0 \n",
    "\n",
    "    for text in augmented_texts:\n",
    "\n",
    "        # Find all of the opening brackets in the text\n",
    "        brackets_open = [m.start() for m in re.finditer(\"\\\\[\", text)]\n",
    "        # Find all of the closing brackets in the text\n",
    "        brackets_close = [m.start() for m in re.finditer(\"]\", text)]\n",
    "\n",
    "        if (len(brackets_open) % 2 > 0) or (len(brackets_close) % 2 > 0):\n",
    "            if not suppress_warnings:\n",
    "                print(f\"An uneven number of brackets has been found! Namely length of BO = {len(brackets_open)} or length of BC = {len(brackets_close)}\")\n",
    "            continue\n",
    "            \n",
    "        if len(brackets_open) != len(brackets_close):\n",
    "            if not suppress_warnings:\n",
    "                print(f\"There is an unequal number of opening and closing brackets! There is {len(brackets_open)} opening brackets, and {len(brackets_close)} closing brackets.\")\n",
    "            continue\n",
    "\n",
    "        # Loop over each tag to create query lines for the entities they annotate, while accounting for nested tags.\n",
    "        for i in range( len(brackets_open) -1 ): # -1 is to prevent the function from finding a next pair of brackets when arriving at the final pair of brackets\n",
    "\n",
    "            # skip this iteration if the previous non-nested (opening) tag has already been investigated\n",
    "            if skip_iteration:\n",
    "                skip_iteration = False\n",
    "                continue\n",
    "            \n",
    "            # skip this iteration if the current tag is part of an already investigated nested structure\n",
    "            if skip_nested_structure:\n",
    "                skip_nested_structure -= 1\n",
    "                continue\n",
    "\n",
    "            io = brackets_open[i] # index of opening bracket of the entity tag that is currently being looped over\n",
    "            ic = brackets_close[i] # index of closing bracket of the entity tag that is currently being looped over\n",
    "            opening_tag = text[io+1:ic] # find the opening tag\n",
    "\n",
    "            # initialise the variables that are required when working with a nested tag\n",
    "            io_next = -1 # index of the opening bracket of the tag that is under investigation in the while statement\n",
    "            ic_next = -1 # index of the closing bracket of the tag that is under investigation in the while statement\n",
    "            ie_tags = [] # initialise a list where the indices of the brackets of the inner entities' tags can be stored\n",
    "                         # ie_tags[0][0] = opening bracket index of ie opening tag; \n",
    "                         # ie_tags[1][1] = closing bracket index of ie closing tag\n",
    "            closing_tag = None # initialise the closing tag that we are trying to find\n",
    "            next_tag = None # initialise the variable that finds the closing tag \n",
    "            # initialise variable that keeps track if the tag that is currently under investigation is a nested one\n",
    "            is_nested = -1 # current tag is nested if is_nested > 0\n",
    "            j = i+1 # initialise index to use in the while statement \n",
    "            \n",
    "            while ( next_tag != opening_tag ) and ( j < len(brackets_open) - 1 ) :\n",
    "                closing_tag = next_tag # Set the closing tag to the tag that was investigated in the previous loop of the while statement\n",
    "                io_next = brackets_open[j] \n",
    "                ic_next = brackets_close[j]\n",
    "                next_tag = text[io_next+1:ic_next]\n",
    "\n",
    "                # Remember the indices of the brackets of the inner entities' tags\n",
    "                ie_tags.append( [io_next, ic_next] ) \n",
    "\n",
    "                is_nested += 1\n",
    "                j += 1\n",
    "\n",
    "            if is_nested > 2 :\n",
    "                if not suppress_warnings:\n",
    "                    print(f\"A tag has been encountered that is nested on more than a single level! It is located in between indices {io_next} and {ic_next}.\")\n",
    "                continue\n",
    "\n",
    "            if is_nested > 0: # if the current tag is a nested tag\n",
    "                # Explanation of used terminology: the inner entity (ie) is the entity that is nested within the outer entity (oe)\n",
    "\n",
    "                # find the indices of the inner entity (ie)\n",
    "                begin_ie = ie_tags[0][1] + 1 # index of the first letter of the inner entity\n",
    "                end_ie = ie_tags[1][0] - 1 # index of the last letter of the inner entity\n",
    "                \n",
    "                # extract the inner entity (ie)\n",
    "                inner_entity = text[ begin_ie: end_ie ].strip()\n",
    "\n",
    "                ie_variable, inner_entity, handleable_ie = modify_str(inner_entity)\n",
    "\n",
    "                # if the unhandable case happens where an entity has length 1, continue\n",
    "                # if not inner_entity:\n",
    "                #     continue\n",
    "\n",
    "                # find the indices of the outer entity (oe)\n",
    "                begin_oe = io + (len(opening_tag) + 2) # index of the first letter of the outer entity\n",
    "                end_oe = brackets_open[i+3] - 2 # index of the last letter of the outer entity\n",
    "                ie_ot = brackets_open[i+1] - 1 # index of the ie opening tag (=ie_ot)\n",
    "                ie_ct = brackets_close[i+2] + 1 # index of the ie closing tag (=ie_ct)\n",
    "                # extract the outer entity (oe)\n",
    "                outer_entity = text[ begin_oe : ie_ot ] + inner_entity + text[ ie_ct : end_oe + 1 ].strip() \n",
    "                \n",
    "                oe_variable, outer_entity, handleable_oe = modify_str(outer_entity)\n",
    "\n",
    "                # if the unhandable case happens where an entity has length 1, continue\n",
    "                if (not handleable_ie) or (not handleable_oe):\n",
    "                    continue\n",
    "                \n",
    "                # Define the variable names for the node labels\n",
    "                oe_node_name = unidecode(oe_variable)\n",
    "                oe_node_name = oe_node_name.replace(\"-\",\"_\")\n",
    "                ie_node_name = unidecode(ie_variable)\n",
    "                ie_node_name = ie_node_name.replace(\"-\",\"_\")\n",
    "                \n",
    "                # check if these variable names are already contained in the set of variable names\n",
    "                if (oe_node_name in node_label_set) or (ie_node_name in node_label_set):\n",
    "                    continue\n",
    "\n",
    "                # Create a Cypher-queriable line that can be used to add this entity to the KG\n",
    "                query_line_oe = f\"({oe_node_name}:{opening_tag}{{name: '{unidecode(outer_entity)}'}})\" \n",
    "                query_set.add(query_line_oe) # add the query line to the set of all query lines that are going to be added to the KG\n",
    "                node_label_set.add(oe_node_name) # add the variable name for the node label to the set of all node labels to prevent duplicates\n",
    "                query_line_ie = f\"({ie_node_name}:{closing_tag}{{name: '{unidecode(inner_entity)}'}})\" \n",
    "                query_set.add(query_line_ie) # add the query line to the set of all query lines that are going to be added to the KG\n",
    "                node_label_set.add(ie_node_name) # add the variable name for the node label to the set of all node labels to prevent duplicates\n",
    "\n",
    "                # Make sure the current for loop skips over the remaining 3 tags from the current nested structure\n",
    "                skip_nested_structure = 3 \n",
    "\n",
    "            else: # if the current tag is not a nested one\n",
    "\n",
    "                # extracting the name of the entity \n",
    "                begin_entity = io + (len(opening_tag) + 2) # index of the first letter of the entity\n",
    "                end_entity = brackets_close[i+1] - (len(opening_tag) + 2) # index of the last letter of the entity\n",
    "                \n",
    "                entity = text[ begin_entity : end_entity  ].strip()\n",
    "\n",
    "                entity_variable, entity, handleable = modify_str(entity)\n",
    "\n",
    "                # if the unhandable case happens where an entity has length 1, continue\n",
    "                if not handleable:\n",
    "                    continue\n",
    "\n",
    "                # Define the variable name for the node label\n",
    "                node_name = unidecode(entity_variable)\n",
    "                node_name = node_name.replace(\"-\",\"_\")\n",
    "\n",
    "                # check if this variable name is already contained in the set of variable names\n",
    "                if node_name in node_label_set:\n",
    "                    continue\n",
    "\n",
    "                # Create a Cypher-queriable line that can be used to add this entity to the KG\n",
    "                query_line = f\"({node_name}:{opening_tag}{{name: '{unidecode(entity)}'}})\" \n",
    "                query_set.add(query_line) # add the query line to the set of all query lines that are going to be added to the KG\n",
    "                node_label_set.add(node_name) # add the variable name for the node label to the set of all node labels to prevent duplicates\n",
    "\n",
    "                # skip the next iteration of the current for loop to prevent a closing tag from being investigated\n",
    "                skip_iteration = True\n",
    "\n",
    "    return query_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running create_entity_query on the first 10 texts of the dataset\n",
    "query_set_NER = create_entity_query(final_texts, suppress_warnings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the set =  34\n",
      "{\"(and_reactor:EMPLOYEE{name: 'and_reactor'})\", \"(mode_2:LOCATION{name: 'mode_2'})\", \"(three/22/88:DATE{name: '3/22/88'})\", \"(two/18/87:DATE{name: '2/18/87'})\", \"(personnel_error:CAUSE{name: 'personnel_error'})\", \"(the_turbine_operator_at_the_turbine_pedestal_used_an_excessive_amount_of_steam_to_increase_turbine_speed:UNEXPECTED EVENT{name: 'the_turbine_operator_at_the_turbine_pedestal_used_an_excessive_amount_of_steam_to_increase_turbine_speed'})\", \"(a_reactor_scram_on_high_main_coolant_pressure:UNEXPECTED EVENT{name: 'a_reactor_scram_on_high_main_coolant_pressure'})\", \"(which_delayed_timely_compensatory_actions_<br_/><br_/>_plant_response_was_normalfollowing_the_scram_corrective_actions_were_taken_to_reinstruct:EMPLOYEE{name: 'which_delayed_timely_compensatory_actions_<br_/><br_/>_plant_response_was_normalfollowing_the_scram_corrective_actions_were_taken_to_reinstruct'})\", \"(operators:EMPLOYEE{name: 'operators'})\", \"(one_hours:TIME{name: '0001_hours'})\", \"(may_1988_all_four_mcps_were_operating:ACTIVITY{name: 'may_1988_all_four_mcps_were_operating'})\", \"(one_hundred_and_one_hours:TIME{name: '0101_hours'})\", \"(a_procedure_inadequacy:CAUSE{name: 'a_procedure_inadequacy'})\", \"(the_reactor_scram:EXPECTED EVENT{name: 'the_reactor_scram'})\", \"(five_hours:TIME{name: '0005_hours'})\", \"(high_main_coolant_pressure_condition:UNEXPECTED EVENT{name: 'high_main_coolant_pressure_condition'})\", \"(two_thousand_three_hundred_and_thirty_hours:TIME{name: '2330_hours'})\", \"(march_26_1988:DATE{name: 'march_26_1988'})\", \"(the_operators:EMPLOYEE{name: 'the_operators'})\", \"(and_establish_a_direct_communications_link_between_the_turbine:EMPLOYEE{name: 'and_establish_a_direct_communications_link_between_the_turbine'})\", \"(four2:TIME{name: '0042'})\", \"(high_main_coolant_pressure:UNEXPECTED EVENT{name: 'high_main_coolant_pressure'})\", \"(operator:EMPLOYEE{name: 'operator'})\", \"(second_factor_was_the:ACTIVITY{name: 'second_factor_was_the'})\", \"(fourteen1:TIME{name: '0141'})\", \"(and_the_reactor:EMPLOYEE{name: 'and_the_reactor'})\", \"(february_18_1987:DATE{name: 'february_18_1987'})\", \"(may_1988:ACTIVITY{name: 'may_1988'})\", \"(a_failure_of_the_field_overvoltage_protection_unit_circuit_board:CAUSE{name: 'a_failure_of_the_field_overvoltage_protection_unit_circuit_board'})\", \"(mode_1:LOCATION{name: 'mode_1'})\", \"(five_hundred_and_twenty_nine_hours:TIME{name: '0529_hours'})\", \"(the_control_room_operators:EMPLOYEE{name: 'the_control_room_operators'})\", \"(plant_response_was_normal:CAUSE{name: 'plant_response_was_normal'})\", \"(automatic_reactor_scram:UNEXPECTED EVENT{name: 'automatic_reactor_scram'})\"}\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "print('length of the set = ', len(query_set_NER))\n",
    "print(query_set_NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Knowledge Graph in Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the RE part of the query -> looping over all 50 texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Database Credentials\n",
    "\n",
    "# uri = \"bolt://localhost:7687\" # Click the copy button in the \"Bolt port\" row from the table that appears when you click NBA_example in Neo4j Desktop\n",
    "# userName = \"neo4j\"\n",
    "# password = \"bilalbroski1\" # password for Text_Mining_Neo4j DBMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Connect to the neo4j database server\n",
    "# graphDB_Driver = GraphDatabase.driver(uri, auth=(userName, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # variable that should be set to \"True\" when the CREATE query has to be run\n",
    "# # The reason why this variable exists, is that if you run the code multiple times \n",
    "# # with create_DB set to True, there will be a lot of duplicates within Neo4j\n",
    "# create_DB = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relation_query(relations_set: set):\n",
    "    \"\"\"\n",
    "    Creates a Cypher query that creates the relation/RE part of a Neo4j Database \n",
    "    \n",
    "    Assumptions: \n",
    "    - relations_set is of shape: set((SUBJECT, relationship, OBJECT), ... ), where SUBJECT and OBJECT are variables \n",
    "        referring to entities and are contained within a tuple.\n",
    "    - THE SUBJECTS, RELATIONSHIPS, AND OBJECTS HAVE THE SAME SHAPE/FORMAT AS IN THE NER MODEL, so\n",
    "        - all have been unidecoded\n",
    "        - subjects and objects have been stripped of leading and trailing whitespaces, are \n",
    "            completely in lowercase, and all leftover whitespaces have been replaced by underscores\n",
    "    \"\"\"\n",
    "    # initialise a set to which all query-strings are added, and that can later be concatenated into a single string.\n",
    "    # This helps preventing duplicates\n",
    "    query_set = set()\n",
    "\n",
    "    for s in relations_set:\n",
    "        \n",
    "        subj = clean_str( s[0].replace(\"'\", \"\") )\n",
    "        relation = s[1]\n",
    "        obj = clean_str( s[2].replace(\"'\", \"\") )\n",
    "\n",
    "        query_line = f\"({ unidecode(subj) })-[:{ relation }]->({ unidecode(obj) })\" \n",
    "        query_set.add(query_line) # add the query line to the set of all query lines that are going to be added to the KG\n",
    "\n",
    "    return query_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_queries(query_set_NER: set, query_set_RE: set):\n",
    "    \"\"\"\n",
    "    Concatenate all of the query lines containing the named entities together with the query lines\n",
    "    containing the relationships, in order to form one final Cypher query that can be ran to \n",
    "    create the KG.\n",
    "    \"\"\"\n",
    "    # Concatenate all of the query lines to form one final Cypher query that creates the KG\n",
    "    cqlCreate = \"\"\"CREATE\"\"\"\n",
    "\n",
    "    # add the entities from the NER set\n",
    "    for i, line1 in enumerate(query_set_NER):\n",
    "        if not i: # the first query entry should not be seperated from the CREATE statement with a comma\n",
    "            cqlCreate = cqlCreate + ' \\n' + line1\n",
    "        else:\n",
    "            cqlCreate = cqlCreate + ',\\n' + line1\n",
    "\n",
    "    # add the relationships from the RE set\n",
    "    for j, line2 in enumerate(query_set_RE):\n",
    "        if j == (len(query_set_RE)-1): # The final entry of the query should end with a semicolon\n",
    "            cqlCreate = cqlCreate + ',\\n' + line2 + ';'\n",
    "        else:\n",
    "            cqlCreate = cqlCreate + ',\\n' + line2\n",
    "\n",
    "    return cqlCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cqlEmpty = \"\"\"match (n) detach delete n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re_output =  [('high main coolant pressure condition', 'caused_by', 'a procedure inadequacy'), ('high main coolant pressure', 'caused_by', 'a procedure inadequacy')]\n",
      "\n",
      "\n",
      "\n",
      "re_output =  [('a reactor scram on high main coolant pressure', 'caused_by', 'personnel error'), ('a reactor scram on high main coolant pressure', 'caused_by', 'Plant response was normal'), ('The turbine operator (at the turbine pedestal) used an excessive amount of steam to increase turbine speed', 'caused_by', 'Plant response was normal'), ('second factor was the', 'done_by', 'operator')]\n",
      "\n",
      "\n",
      "\n",
      "re_output =  []\n",
      "\n",
      "\n",
      "\n",
      "re_output =  [('The reactor scram', 'happened_during', '0529 hours')]\n",
      "\n",
      "\n",
      "\n",
      "re_output =  [('May 1988,', 'happened_during', 'Mode 1')]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialise set to add all outputs of the RE model to\n",
    "RE_output_set = set()\n",
    "\n",
    "RE_list = []\n",
    "\n",
    "# Loop over all 50 texts\n",
    "for text_id in range(5):\n",
    "\n",
    "    RE_sublist = []\n",
    "\n",
    "    # import the output from the RE model\n",
    "    RE_data_path = f\"C:/Users/guusj/Documents/AAA_Master_DSAI/Y2Q1/2AMM30_Text_Mining/RESIT/Data Ivan/test_output_C2/file_{text_id}\"\n",
    "    with open(RE_data_path, \"rb\") as fp:\n",
    "        re_output = pickle.load(fp)\n",
    "        \n",
    "        print('re_output = ', re_output)\n",
    "\n",
    "        # preprocess the variable names of the entities within data. \n",
    "        # The variable names of the relationships already exist in the correct form\n",
    "        for i in range(len(re_output)):\n",
    "            old_relation = re_output[i]\n",
    "\n",
    "            # perform preprocessing\n",
    "            new_relation_0 = old_relation[0].strip().replace(\" \", \"_\").lower()\n",
    "            new_relation_2 = old_relation[2].strip().replace(\" \", \"_\").lower()\n",
    "\n",
    "            # Make sure the imported variable names comply with the naming restrcitions of Cypher\n",
    "            new_relation_0, handleable0 = fix_var_name(new_relation_0)\n",
    "            new_relation_0 = unidecode(new_relation_0)\n",
    "            new_relation_2, handleable2 = fix_var_name(new_relation_2)\n",
    "            new_relation_2 = unidecode(new_relation_2)\n",
    "\n",
    "            if (handleable0 and handleable2):\n",
    "                # Combine all new entries into a tuple and use it to replace the old tuple\n",
    "                new_relation = (new_relation_0, old_relation[1], new_relation_2)                \n",
    "                re_output[i] = new_relation\n",
    "\n",
    "                # convert the re_output list into a set\n",
    "                # re_output = set(re_output)\n",
    "                \n",
    "                # add re_output to RE_output_set\n",
    "                RE_output_set.add(new_relation)\n",
    "                RE_sublist.append(new_relation)\n",
    "                # print('RE_sublist = ',RE_sublist)\n",
    "            else:\n",
    "                print(\"KAAAAAAAAAAAAAAAAS \", i)\n",
    "\n",
    "    # STOP THE LOOP FOR EACH TEXT, SUCH THAT YOU CAN SCREENSHOT THE ACCOMPANYING KNOWLEDGE GRAPH IN NEO4J\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    query_set_RE = create_relation_query(RE_output_set)\n",
    "    RE_list.append(RE_sublist)\n",
    "    # print(f'query_set_RE text nr {text_id}: ', query_set_RE)\n",
    "    # print(\"RE_list = \", RE_list)\n",
    "    print()\n",
    "    \n",
    "    # cqlCreate = combine_queries(query_set_NER, query_set_RE)\n",
    "    # print(f'cqlCreate text nr {text_id}' , cqlCreate)\n",
    "    # print()\n",
    "\n",
    "    # # Execute the CQL query to create the KG\n",
    "    # if create_DB:\n",
    "    #     with graphDB_Driver.session() as graphDB_Session:\n",
    "    #         # Create nodes\n",
    "    #         graphDB_Session.run(cqlCreate)\n",
    "\n",
    "    # input(\"Press Enter to continue...\")\n",
    "    # print('continue after text_id nr ', text_id)\n",
    "    # # Execute the CQL query to empty the KG\n",
    "    # if create_DB:\n",
    "    #     with graphDB_Driver.session() as graphDB_Session:\n",
    "    #         # Create nodes\n",
    "    #         graphDB_Session.run(cqlEmpty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(RE_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('high_main_coolant_pressure_condition',\n",
       "   'caused_by',\n",
       "   'a_procedure_inadequacy'),\n",
       "  ('high_main_coolant_pressure', 'caused_by', 'a_procedure_inadequacy')],\n",
       " [('a_reactor_scram_on_high_main_coolant_pressure',\n",
       "   'caused_by',\n",
       "   'personnel_error'),\n",
       "  ('a_reactor_scram_on_high_main_coolant_pressure',\n",
       "   'caused_by',\n",
       "   'plant_response_was_normal'),\n",
       "  ('the_turbine_operator_(at_the_turbine_pedestal)_used_an_excessive_amount_of_steam_to_increase_turbine_speed',\n",
       "   'caused_by',\n",
       "   'plant_response_was_normal'),\n",
       "  ('second_factor_was_the', 'done_by', 'operator')],\n",
       " [],\n",
       " [('the_reactor_scram',\n",
       "   'happened_during',\n",
       "   'five_hundred_and_twenty_nine_hours')],\n",
       " [('may_1988,', 'happened_during', 'mode_1')]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RE_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if create_DB:\n",
    "#     with graphDB_Driver.session() as graphDB_Session:\n",
    "#         # Create nodes\n",
    "#         graphDB_Session.run(cqlEmpty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE_output_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(a_reactor_scram_on_high_main_coolant_pressure)-[:caused_by]->(personnel_error)',\n",
       " '(a_reactor_scram_on_high_main_coolant_pressure)-[:caused_by]->(plant_response_was_normal)',\n",
       " '(high_main_coolant_pressure)-[:caused_by]->(a_procedure_inadequacy)',\n",
       " '(high_main_coolant_pressure_condition)-[:caused_by]->(a_procedure_inadequacy)',\n",
       " '(may_1988)-[:happened_during]->(mode_1)',\n",
       " '(second_factor_was_the)-[:done_by]->(operator)',\n",
       " '(the_reactor_scram)-[:happened_during]->(five_hundred_and_twenty_nine_hours)',\n",
       " '(the_turbine_operator_at_the_turbine_pedestal_used_an_excessive_amount_of_steam_to_increase_turbine_speed)-[:caused_by]->(plant_response_was_normal)'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_set_RE = create_relation_query(RE_output_set)\n",
    "# Show results\n",
    "query_set_RE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Final Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cqlCreate = combine_queries(query_set_NER, query_set_RE)\n",
    "# # Show results\n",
    "# cqlCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Knowledge Graph in Neo4j using Cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Database Credentials\n",
    "\n",
    "# uri = \"bolt://localhost:7687\" # Click the copy button in the \"Bolt port\" row from the table that appears when you click NBA_example in Neo4j Desktop\n",
    "# userName = \"neo4j\"\n",
    "# password = \"bilalbroski1\" # password for Text_Mining_Neo4j DBMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Connect to the neo4j database server\n",
    "# graphDB_Driver = GraphDatabase.driver(uri, auth=(userName, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # variable that should be set to \"True\" when the CREATE query has to be run\n",
    "# # The reason why this variable exists, is that if you run the code multiple times \n",
    "# # with create_DB set to True, there will be a lot of duplicates within Neo4j\n",
    "# create_DB = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a few queries to test the Knowledge Graph with after it has been created\n",
    "\n",
    "# # CQL (=Cypher Query Language) to query all players that played for the Dutch national team\n",
    "# cqlNationalTeamQuery = \"\"\"MATCH (player:PLAYER) -[:played_for] -> (country:COUNTRY) \n",
    "# WHERE country.name = \"Netherlands\"\n",
    "# RETURN player.name\n",
    "# \"\"\"\n",
    "# # CQL (=Cypher Query Language) to query all players that play as goalkeeper\n",
    "# cqlGoalkeeperQuery = \"\"\"MATCH (player:PLAYER) -[:plays_as] -> (position:POSITION) \n",
    "# WHERE position.name = \"Goalkeeper\"\n",
    "# RETURN player.name\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cqlCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Execute the CQL query to create the KG\n",
    "# if create_DB:\n",
    "#     with graphDB_Driver.session() as graphDB_Session:\n",
    "#         # Create nodes\n",
    "#         graphDB_Session.run(cqlCreate)\n",
    "\n",
    "# # Show the query that we ran again\n",
    "# cqlCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Execute all other CQL queries and print the results\n",
    "# with graphDB_Driver.session() as graphDB_Session:\n",
    "#     # Query the graph #1\n",
    "#     dutch_players = graphDB_Session.run(cqlNationalTeamQuery)\n",
    "\n",
    "#     print(\"Names of all football players that have played for the Dutch national team:\")\n",
    "#     for player in dutch_players:\n",
    "#         print(player)\n",
    "\n",
    "#     # Query the graph #2\n",
    "#     goalkeepers = graphDB_Session.run(cqlGoalkeeperQuery)\n",
    "\n",
    "#     print('\\n')\n",
    "#     print(\"Names of players that play as goalkeepers:\")\n",
    "#     for player in goalkeepers:\n",
    "#         print(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix with helpful explanations / code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of helpful Cypher commands:\n",
    "- To show the complete KG: \n",
    "    - MATCH (n) RETURN n\n",
    "- To delete the complete KG:\n",
    "    - MATCH (n) DETACH DELETE n\n",
    "\n",
    "For an example of what a Python query to create a KG in Cypher/Neo4j should look like: https://github.com/harblaith7/Neo4j-Crash-Course/blob/main/01-initial-data.cypher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find index:\n",
    "\n",
    "If you want to know which index a certain text has within the dataset,you can enter the name that starts the text as a string to the begin_str variable.\n",
    "\n",
    "\n",
    "The following code will then print the index of the text that begins like this when you run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the number of players/texts\n",
    "# nr_texts = 1031 # set to max number of texts (1031) since you want to search all of these\n",
    "\n",
    "# # begin_str = \"Thomas Alun Lockyer\"\n",
    "# begin_str = \"Mark Maria Hubertus Flekken\"\n",
    "\n",
    "# for i in range(nr_texts):\n",
    "#     with open(f'./data/footballer_{i}.txt', 'r') as f:\n",
    "#         contents = f.read()\n",
    "#         # texts.append(contents)\n",
    "#         if (contents[:len(begin_str)] == begin_str):\n",
    "#             print(f\"The index of the text that starts with '{begin_str}' is: \", i)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2amm30",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
