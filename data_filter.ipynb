{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bilal\\.conda\\envs\\textMining\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\Bilal\\.conda\\envs\\textMining\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "c:\\Users\\Bilal\\.conda\\envs\\textMining\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL for the Premier League players page\n",
    "base_url = \"https://www.worldfootball.net/players_list/eng-premier-league-2023-2024/nach-name/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store all players\n",
    "all_players = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the URLs and send HTTP GET requests\n",
    "for i in range(1, 13):\n",
    "    url = base_url + str(i) + \"/\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        table = soup.find('table', class_='standard_tabelle')\n",
    "        \n",
    "        if table:\n",
    "            # Find all the table rows (tr) within the table with class 'standard_tabelle'\n",
    "            rows = table.find_all('tr')\n",
    "            \n",
    "            # Loop through the rows and extract player names\n",
    "            for row in rows:\n",
    "                # Check if the row contains a link (a) with player information\n",
    "                player_link = row.find('a', href=True)\n",
    "                \n",
    "                if player_link:\n",
    "                    # Extract the player name and add it to the list\n",
    "                    player_name = player_link.get_text()\n",
    "                    all_players.append(player_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all strings to lower case\n",
    "all_players = list(map(lambda x: x.lower(), all_players))\n",
    "#remove whitespace in front and after the names\n",
    "all_players = list(map(lambda x: x.strip(), all_players))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "print(len(all_players))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directories containing wikipedia articles\n",
    "dirA = r'C:\\Users\\Bilal\\Desktop\\TUe\\Y2\\Q1\\TM\\enwiki20230820-stripped-json\\enwiki20230820-stripped-json\\AA'\n",
    "dirB = r'C:\\Users\\Bilal\\Desktop\\TUe\\Y2\\Q1\\TM\\enwiki20230820-stripped-json\\enwiki20230820-stripped-json\\AB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate list that will contain the wikipedia articles about the players\n",
    "player_articles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over all files and selecting the lines whereby the title equals the name of the football players\n",
    "for dir in [dirA, dirB]:\n",
    "    for file in os.listdir(dir):\n",
    "        with open(dir+'\\\\'+file, 'r') as fp:\n",
    "            while True:\n",
    "                line = fp.readline()\n",
    "                if line == '':\n",
    "                    fp.close()\n",
    "                    break\n",
    "                line = json.loads(line)\n",
    "                title = line['title'].lower().strip()\n",
    "                if title in all_players:\n",
    "                    player_articles.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(player_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only articles which are not empty\n",
    "player_articles = list(filter(lambda x: x['text'] != '', player_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(player_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only articles if they contain football or soccer in the text\n",
    "player_articles = list(filter(lambda x: 'footballer' in x['text'] or 'soccer player' in x['text'], player_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(player_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_used = [i[:-4] for i in os.listdir(r'football_articles')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(already_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_articles = list(filter(lambda x: x['title'] not in already_used, player_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(player_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "index = 0\n",
    "while count <= 35:\n",
    "    title = player_articles[index]['title']\n",
    "    n_words = player_articles[index]['text'].count(' ')\n",
    "    if n_words > 50:\n",
    "        with open(r'to_be_annotated\\\\test\\\\'+title+'.txt', 'w') as fp:\n",
    "            text = player_articles[index]['text'].split(' ')[:min([200, n_words])]\n",
    "            new_text = ' '.join(text)\n",
    "            new_text = new_text.replace('\\n', ' ')\n",
    "            fp.write(new_text)\n",
    "            fp.close()\n",
    "            count += 1\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "index = 0\n",
    "while count <= 175:\n",
    "    title = player_articles[index]['title']\n",
    "    if title+'.txt' in os.listdir(r'to_be_annotated\\\\test'):\n",
    "        index += 1\n",
    "        continue\n",
    "    else:\n",
    "        n_words = player_articles[index]['text'].count(' ')\n",
    "        if n_words > 50:\n",
    "            with open(r'to_be_annotated\\\\train\\\\'+title+'.txt', 'w') as fp:\n",
    "                text = player_articles[index]['text'].split(' ')[:min([200, n_words])]\n",
    "                new_text = ' '.join(text)\n",
    "                new_text = new_text.replace('\\n', ' ')\n",
    "                fp.write(new_text)\n",
    "                fp.close()\n",
    "        count += 1\n",
    "        index += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
